<div class="row">
  <div class="col-md-offset-1 col-md-10 post">
  	<h1>AngularJS: How to create a SPA crawlable and SEO friendly?</h1>
  	<p class="date">AUG 23, 2015</p>

  	<div>

 
<h2 id="what-is-spa-">What is SPA?</h2>
<p>SPA stands for <em>Single-Page Application</em>. Its usage is growing in a fast pace since it enables the website to provide a more fluid user experience. When a page is loaded, you can browse other links and load new content without the need of a full page reload. Only one piece of the page is discarded and replaced by the content of the next page. With this, you will avoid requesting all JavaScript and CSS files again and rendering your header and footer HTML.</p>
<p>For example, this blog was created using the Angular ngRoute directive. If you try to change to another blog post, you can see that the header content will not flash and the post will load very fast.</p>
<p>However, this approach have some <a href="http://stackoverflow.com/q/21862054/1476885">drawbacks</a>, but all of them can be handled in some way. What I&#39;m focusing in this post is how to help the search engines, like Google, to properly index your content and make it searchable.</p>
<h2 id="seo-and-crawlable-pages">SEO and Crawlable Pages</h2>
<p>SEO stands for <em>Search Engine Optimization</em>. What we are trying to achieve here is a way to help search engines to render your page and understand your content. </p>
<p>Search engines uses web crawlers to systematically browse web pages to collect information and to index them based on its contents. The main issue here is that most of the web crawlers don&#39;t execute JavaScript code and can&#39;t see what your SPA retrieves with each link. I&#39;ve said <strong>most</strong> web crawlers. Fortunately, <a href="http://googlewebmastercentral.blogspot.com.br/2014/05/understanding-web-pages-better.html">Google is currently prepared</a> to execute JavaScript code. This make your task much easier, as you can see in the following section.</p>
<h2 id="crawlable-pages-for-google">Crawlable Pages for Google</h2>
<p>If you believe that being googleable is enough for your website, then all you need is to add a <a href="http://www.sitemaps.org/protocol.html">Sitemap</a> file named <strong>sitemap.xml</strong> at your web server root directory to tell the Google robots what links do you have in your website. Strictly speaking, even this step would not be necessary since Google executes JavaScript and is able to find hidden links, but <a href="http://googlewebmastercentral.blogspot.com.br/2014/05/understanding-web-pages-better.html">they admit</a> that some JavaScript code may not be executed if its too complex or arcane. </p>
<p>So, adding a Sitemap is a way to help Google to find your links and is very easy to be accomplished. Also, note that you can include &quot;invisible links&quot;, like the ones that just appears after you fill a form.</p>
<p>In this blog, I&#39;ve added the following <strong>sitemap.xml</strong> file:</p>
<pre><code class="lang-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;urlset xmlns=&quot;http://www.sitemaps.org/schemas/sitemap/0.9&quot;&gt;
  &lt;url&gt;
    &lt;loc&gt;http://zanon.io/&lt;/loc&gt;
    &lt;lastmod&gt;2015-08-23&lt;/lastmod&gt;
    &lt;changefreq&gt;weekly&lt;/changefreq&gt;
  &lt;/url&gt;
  &lt;url&gt;
    &lt;loc&gt;http://zanon.io/angular-how-to-create-a-spa-crawlable-and-seo-friendly&lt;/loc&gt;
    &lt;lastmod&gt;2015-08-23&lt;/lastmod&gt;
    &lt;changefreq&gt;monthly&lt;/changefreq&gt;
  &lt;/url&gt;  
&lt;/urlset&gt;
</code></pre>
<p>Note: if you want your site to be correctly rendered in Facebook&#39;s preview or Twitter&#39;s cards, then you need to pre-render your pages. In this case, been just googleable is not enough. Continue reading for more.</p>
<h2 id="google-crawler-and-hosting-in-amazon-s3">Google Crawler and Hosting in Amazon S3</h2>
<p>I&#39;ve blogged <a href="http://zanon.io/angularjs-with-pretty-urls-removing-the-in-amazon-s3">here</a> about how to get pretty URLs removing the hash <code>#</code> sign. If you&#39;ve done that and is using Amazon S3 to host your SPA, then Google will not be able to find your pages automatically.</p>
<p>The problem is that to remove the hash <code>#</code> sign, you need to use Angular&#39;s html5Mode and it requires a server rewrite rule to enable the server to show pages where they don&#39;t exist. For example, there is no <em>about</em> directory with HTML files when a user browses for <code>http://mywebsite.com/about</code> thus a 404 error will be returned. To avoid that, the URL rewrite rule will serve the index.html file, but Angular will receive the <code>http://mywebsite.com/about</code> URL to render it correctly.</p>
<p>However, Amazon S3 is not a web server and lacks this server rewrite option. What we do is to capture 404 errors and <strong>redirect</strong> them to a similar URL but with the hash <code>#</code> sign added, like in <code>http://mywebsite.com/#/about</code>. With this hash <code>#</code> sign and Angular html5Mode enabled, the URL will be changed again to <code>http://mywebsite.com/about</code> and will be correctly rendered. So, the problem is in this <strong>redirect</strong>. I&#39;ve tested this and Google will not follow and index your page.</p>
<p>In this case, you&#39;ll need to pre-render your pages and use the Sitemap to link to them. You will se how this can be done reading the following sections.</p>
<h2 id="crawlable-pages-for-bing-and-others">Crawlable Pages for Bing and Others</h2>
<p>For crawlers that aren&#39;t able to execute JavaScript code, you need to pre-render the page and make it available for them. Also, the Sitemap should reference those pre-rendered pages.</p>
<p>The first step is to identify your URL design and <a href="https://developers.google.com/webmasters/ajax-crawling/docs/learn-more">follow some conventions</a>. Are you using the hash <code>#</code> sign or not?</p>
<h3 id="what-to-change-when-your-urls-use-the-hash-sign">What to change when your URLs use the hash sign</h3>
<p>If you use the hash <code>#</code> sign in your URLs, you will need to change it to the hashbang <code>#!</code> sign using the following code:</p>
<pre><code class="lang-javascript">angular.module(&#39;myApp&#39;).config(function($locationProvider) {
    $locationProvider.hashPrefix(&#39;!&#39;);
});
</code></pre>
<p>Using this approach you&#39;ll get links like the following: <code>www.example.com/#!contact</code></p>
<p>When the crawler sees this link, it will rewrite the URL and send a GET to the server using the following URL: <code>www.example.com/?_escaped_fragment_=contact</code></p>
<h3 id="what-to-change-when-you-have-pretty-urls">What to change when you have pretty URLs</h3>
<p>If instead of using those ugly URLs you have already adopted the HTML5 push state, you will need first to add the following meta tag to the page <code>&lt;head&gt;</code>:</p>
<pre><code class="lang-html">&lt;meta name=&quot;fragment&quot; content=&quot;!&quot;&gt;
</code></pre>
<p>With this configuration, crawlers will see the URL <code>www.example.com/contact</code>, but send GET requests to <code>www.example.com/contact?_escaped_fragment_=</code></p>
<h3 id="sitemap">Sitemap</h3>
<p>In you Sitemap you can use the normal user links or you can use the parsed links (<code>?_escaped_fragment_=</code>), but prefer the former because <a href="https://developers.google.com/webmasters/ajax-crawling/docs/faq?hl=en#question-what-url-should-i-put-in-my-sitemap">which kind you choose in your Sitemap is what the search engine will show as result</a>.</p>
<h3 id="pre-rendering">Pre-rendering</h3>
<p>The last step is to pre-render your pages and prepare your server to handle those modified requests. For this step, there are two solutions that are widely used:</p>
<ol>
<li><p><a href="https://prerender.io/">Prerender.io</a>: this framework acts like a middleware checking each request to see if it&#39;s a request from a crawler. If it is, Prerender.io will generate a static HTML version of the page and send it as the result. You can read more about how to use it <a href="https://prerender.io/documentation">here</a> and <a href="https://scotch.io/tutorials/angularjs-seo-with-prerender-io">here</a>.</p>
</li>
<li><p><a href="http://phantomjs.org/">PhantomJS</a>: is a headless browser that can be used to GET a page URL and output its rendered HTML result. The solution here would be to automate this task so every time you make a new commit, a PhantomJS task would be called to render the HTML pages and store it to be available to crawlers. Or you can dynamically render the page when a crawler request it.  You can read more about how to use it <a href="http://phantomjs.org/documentation/">here</a> and <a href="http://lawsonry.com/2014/05/diy-angularjs-seo-with-phantomjs-the-easy-way/">here</a>.</p>
</li>
</ol>
<p>If you&#39;re hosting a static website in Amazon S3, you can&#39;t use a middleware solution like Prerender.io, so you have to stick with PhantomJS. I&#39;ve created a tutorial for this and its available <a href="http://zanon.io/using-phantomjs-and-amazon-s3-to-serve-pre-rendered-content">here</a>.</p>


    </div>
  </div>
</div>